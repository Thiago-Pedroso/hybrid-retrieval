{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20c8e13",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04a400d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Adicione o path do projeto\n",
    "sys.path.insert(0, str(Path(\".\").resolve()))\n",
    "\n",
    "# Importe suas classes\n",
    "from src.datasets.schema import Document, Query\n",
    "from src.retrievers.tfidf_faiss import TFIDFRetriever\n",
    "\n",
    "\n",
    "# Ajuste caso tenha salvo em outro lugar:\n",
    "ROOT = Path(\"./data\").resolve()\n",
    "\n",
    "# Conjuntos BEIR do paper:\n",
    "DATASETS = [\"scifact\", \"fiqa\", \"nfcorpus\"]\n",
    "TOPK = 100  # top-k para avalia√ß√£o\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1429115f",
   "metadata": {},
   "source": [
    "## Utils I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07ef93bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_or_jsonl(path_parquet: Path, path_jsonl: Path) -> pd.DataFrame:\n",
    "    if path_parquet.exists():\n",
    "        return pd.read_parquet(path_parquet)\n",
    "    if path_jsonl.exists():\n",
    "        return pd.read_json(path_jsonl, lines=True)\n",
    "    raise FileNotFoundError(f\"Faltam arquivos: {path_parquet} | {path_jsonl}\")\n",
    "\n",
    "def load_beir_processed(ds_name: str):\n",
    "    base = ROOT / ds_name / \"processed\" / \"beir\"\n",
    "    paths = {\n",
    "        \"corpus\":  (base / \"corpus.parquet\",  base / \"corpus.jsonl\"),\n",
    "        \"queries\": (base / \"queries.parquet\", base / \"queries.jsonl\"),\n",
    "        \"qrels\":   (base / \"qrels.parquet\",   base / \"qrels.jsonl\"),\n",
    "    }\n",
    "    df_corpus  = load_parquet_or_jsonl(*paths[\"corpus\"])\n",
    "    df_queries = load_parquet_or_jsonl(*paths[\"queries\"])\n",
    "    df_qrels   = load_parquet_or_jsonl(*paths[\"qrels\"])\n",
    "\n",
    "    # Normaliza√ß√µes leves\n",
    "    df_corpus[\"doc_id\"]   = df_corpus[\"doc_id\"].astype(str)\n",
    "    df_queries[\"query_id\"] = df_queries[\"query_id\"].astype(str)\n",
    "    if \"split\" not in df_qrels.columns:\n",
    "        df_qrels[\"split\"] = \"test\"  # fallback\n",
    "    df_qrels[\"query_id\"] = df_qrels[\"query_id\"].astype(str)\n",
    "    df_qrels[\"doc_id\"]   = df_qrels[\"doc_id\"].astype(str)\n",
    "    if \"score\" not in df_qrels.columns:\n",
    "        df_qrels[\"score\"] = 1\n",
    "\n",
    "    return df_corpus, df_queries, df_qrels\n",
    "\n",
    "def pick_split_available(qrels: pd.DataFrame, prefer=\"test\"):\n",
    "    order = [prefer, \"dev\", \"validation\", \"train\"]\n",
    "    present = set(qrels[\"split\"].unique().tolist())\n",
    "    for s in order:\n",
    "        if s in present:\n",
    "            return s\n",
    "    return qrels[\"split\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dca84b",
   "metadata": {},
   "source": [
    "## Tokeniza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48ed655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza√ß√£o simples\n",
    "_tok_re = re.compile(r\"[A-Za-z0-9_]+\")\n",
    "def tokenize(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "    return [t.lower() for t in _tok_re.findall(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543254c",
   "metadata": {},
   "source": [
    "## M√©tricas (Top-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8efc34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tricas\n",
    "def mrr_at_k(ranked, gold_set, k=10):\n",
    "    for i, did in enumerate(ranked[:k], start=1):\n",
    "        if did in gold_set:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def dcg_at_k(ranked, gains, k=10):\n",
    "    dcg = 0.0\n",
    "    for i, did in enumerate(ranked[:k], start=1):\n",
    "        g = gains.get(did, 0.0)\n",
    "        if g > 0:\n",
    "            dcg += (2**g - 1) / np.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(ranked, gains, k=10):\n",
    "    ideal = sorted(gains.values(), reverse=True)[:k]\n",
    "    idcg = 0.0\n",
    "    for i, g in enumerate(ideal, start=1):\n",
    "        idcg += (2**g - 1) / np.log2(i + 1)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(ranked, gains, k) / idcg\n",
    "\n",
    "def average_precision_at_k(ranked, gold_set, k=10):\n",
    "    hits, s = 0, 0.0\n",
    "    for i, did in enumerate(ranked[:k], start=1):\n",
    "        if did in gold_set:\n",
    "            hits += 1\n",
    "            s += hits / i\n",
    "    return 0.0 if not gold_set else s / min(len(gold_set), k)\n",
    "\n",
    "def recall_at_k(ranked, gold_set, k=10):\n",
    "    if not gold_set:\n",
    "        return 0.0\n",
    "    return len(set(ranked[:k]) & gold_set) / len(gold_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c275ee6",
   "metadata": {},
   "source": [
    "## √≠ndice TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00c7eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_index(df_corpus: pd.DataFrame, dataset_name: str):\n",
    "    \"\"\"Constr√≥i √≠ndice TF-IDF usando sua classe TFIDFRetriever\"\"\"\n",
    "    # Converte DataFrame para Document objects\n",
    "    documents = []\n",
    "    for _, row in df_corpus.iterrows():\n",
    "        doc = Document(\n",
    "            doc_id=str(row[\"doc_id\"]),\n",
    "            title=str(row.get(\"title\", \"\") or \"\"),\n",
    "            text=str(row.get(\"text\", \"\") or \"\")\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Cria retriever com cache em artifacts\n",
    "    artifact_dir = f\"./outputs/artifacts/{dataset_name}_baseline_tfidf\"\n",
    "    retriever = TFIDFRetriever(\n",
    "        dim=1000,\n",
    "        min_df=2,\n",
    "        backend=\"sklearn\",\n",
    "        use_faiss=True,\n",
    "        artifact_dir=artifact_dir,\n",
    "        index_name=\"tfidf.index\"\n",
    "    )\n",
    "    \n",
    "    # Constr√≥i √≠ndice (ou carrega do cache se existir)\n",
    "    retriever.build_index(documents)\n",
    "    \n",
    "    return retriever, documents\n",
    "\n",
    "def rank_with_tfidf(retriever: TFIDFRetriever, query_text: str, topk=TOPK):\n",
    "    \"\"\"Ranking usando TFIDFRetriever\"\"\"\n",
    "    # Cria objeto Query\n",
    "    query = Query(query_id=\"tmp\", text=query_text)\n",
    "    \n",
    "    # Recupera top-k\n",
    "    results = retriever.retrieve([query], k=topk)\n",
    "    \n",
    "    # Extrai doc_ids e scores\n",
    "    ranked_items = results.get(\"tmp\", [])\n",
    "    doc_ids = [doc_id for doc_id, score in ranked_items]\n",
    "    scores = [score for doc_id, score in ranked_items]\n",
    "    \n",
    "    return doc_ids, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de840f",
   "metadata": {},
   "source": [
    "## Loop de avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb8ed9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(name: str, topk=TOPK, only_test=True, show_examples=3):\n",
    "    print(f\"\\n====== {name} ======\")\n",
    "    df_corpus, df_queries, df_qrels = load_beir_processed(name)\n",
    "    print(\"corpus:\", df_corpus.shape, \"| queries:\", df_queries.shape, \"| qrels:\", df_qrels.shape)\n",
    "\n",
    "    desired = \"test\" if only_test else \"test\"\n",
    "    split = pick_split_available(df_qrels, prefer=desired)\n",
    "    if only_test and split != \"test\":\n",
    "        print(f\"‚ö†Ô∏è Split 'test' n√£o encontrado em {name}; usando '{split}' para rodar mesmo assim.\")\n",
    "    qrels_split = df_qrels[df_qrels[\"split\"] == split].copy()\n",
    "    print(\"usando split:\", split, \"| qrels:\", qrels_split.shape)\n",
    "\n",
    "    # mapa query_id -> {doc_id: score}\n",
    "    qrels_map = {}\n",
    "    for row in qrels_split.itertuples(index=False):\n",
    "        qrels_map.setdefault(row.query_id, {})[row.doc_id] = int(getattr(row, \"score\", 1))\n",
    "\n",
    "    # üÜï Constr√≥i √≠ndice TF-IDF usando sua classe\n",
    "    retriever, documents = build_tfidf_index(df_corpus, name)\n",
    "\n",
    "    # prepara lookup de queries existentes\n",
    "    qdf = df_queries[df_queries[\"query_id\"].isin(qrels_split[\"query_id\"].unique())]\n",
    "    q_lookup = dict(zip(qdf[\"query_id\"], qdf[\"query\"]))\n",
    "\n",
    "    metrics = {\"MRR@10\": [], \"nDCG@10\": [], \"MAP@10\": [], \"Recall@10\": []}\n",
    "    examples = []\n",
    "\n",
    "    for qid, gold_gains in tqdm(qrels_map.items(), desc=f\"TF-IDF {name} ({split})\"):\n",
    "        qtxt = q_lookup.get(qid)\n",
    "        if qtxt is None:\n",
    "            continue\n",
    "        gold_set = {d for d, s in gold_gains.items() if s > 0}\n",
    "        \n",
    "        # üÜï Ranking com TFIDFRetriever\n",
    "        ranked, _ = rank_with_tfidf(retriever, qtxt, topk=topk)\n",
    "\n",
    "        metrics[\"MRR@10\"].append(mrr_at_k(ranked, gold_set, k=10))\n",
    "        metrics[\"nDCG@10\"].append(ndcg_at_k(ranked, gold_gains, k=10))\n",
    "        metrics[\"MAP@10\"].append(average_precision_at_k(ranked, gold_set, k=10))\n",
    "        metrics[\"Recall@10\"].append(recall_at_k(ranked, gold_set, k=10))\n",
    "\n",
    "    # agrega\n",
    "    results = {m: float(np.mean(v)) if v else 0.0 for m, v in metrics.items()}\n",
    "    print(\"Resultados (m√©dias):\", results)\n",
    "\n",
    "    # exemplos qualitativos\n",
    "    for qid in list(qrels_map.keys())[:show_examples]:\n",
    "        qtxt = q_lookup.get(qid)\n",
    "        if not qtxt:\n",
    "            continue\n",
    "        ranked, _ = rank_with_tfidf(retriever, qtxt, topk=10)\n",
    "        gold = {d for d, s in qrels_map[qid].items() if s > 0}\n",
    "        hit_rank = next((i+1 for i, did in enumerate(ranked) if did in gold), None)\n",
    "        # t√≠tulo do top-1\n",
    "        top1 = ranked[0] if ranked else None\n",
    "        title_top1 = df_corpus.loc[df_corpus[\"doc_id\"] == top1, \"title\"].head(1).tolist()\n",
    "        title_top1 = title_top1[0] if title_top1 else None\n",
    "        examples.append({\n",
    "            \"query_id\": qid,\n",
    "            \"query\": qtxt,\n",
    "            \"gold_size\": len(gold),\n",
    "            \"hit@10_rank\": hit_rank,\n",
    "            \"top1_doc_id\": top1,\n",
    "            \"top1_title\": title_top1\n",
    "        })\n",
    "    ex_df = pd.DataFrame(examples)\n",
    "    if not ex_df.empty:\n",
    "        print(\"\\nAmostras de consultas (top-1 e acerto@10):\")\n",
    "        display(ex_df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "304a6116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== scifact ======\n",
      "corpus: (5183, 4) | queries: (1109, 2) | qrels: (1258, 4)\n",
      "usando split: test | qrels: (339, 4)\n",
      "2025-11-01 21:02:33 | INFO     | retriever.tfidf | [tfidf_faiss.py:66] | üöÄ Building TF-IDF Index (5183 documentos)\n",
      "2025-11-01 21:02:33 | INFO     | retriever.tfidf | [logging.py:199] | ‚è±Ô∏è  Fit TF-IDF no corpus - iniciando...\n",
      "2025-11-01 21:02:33 | INFO     | tfidf.vectorizer | [logging.py:199] | ‚è±Ô∏è  Fit TF-IDF - iniciando...\n",
      "2025-11-01 21:02:33 | INFO     | tfidf.vectorizer | [logging.py:220] | ‚úì Fit TF-IDF - conclu√≠do em \u001b[32m333.8ms\u001b[0m\n",
      "2025-11-01 21:02:33 | INFO     | tfidf.vectorizer | [tfidf_vectorizer.py:19] | ‚úì TF-IDF fitted: vocab_size=1000\n",
      "2025-11-01 21:02:33 | INFO     | retriever.tfidf | [logging.py:220] | ‚úì Fit TF-IDF no corpus - conclu√≠do em \u001b[32m336.3ms\u001b[0m\n",
      "2025-11-01 21:02:33 | INFO     | retriever.tfidf | [logging.py:199] | ‚è±Ô∏è  Encoding documents (TF-IDF) - iniciando...\n",
      "2025-11-01 21:02:34 | INFO     | retriever.tfidf | [logging.py:220] | ‚úì Encoding documents (TF-IDF) - conclu√≠do em \u001b[32m584.8ms\u001b[0m\n",
      "2025-11-01 21:02:34 | INFO     | retriever.tfidf | [tfidf_faiss.py:78] |   ‚úì √çndice carregado do cache: outputs/artifacts/scifact_baseline_tfidf/tfidf.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF scifact (test):   0%|          | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m all_results = {}\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m DATASETS:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     all_results[ds] = \u001b[43mevaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_test\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m== Resumo (m√©dias por dataset) ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m display(pd.DataFrame(all_results).T)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mevaluate_dataset\u001b[39m\u001b[34m(name, topk, only_test, show_examples)\u001b[39m\n\u001b[32m     32\u001b[39m gold_set = {d \u001b[38;5;28;01mfor\u001b[39;00m d, s \u001b[38;5;129;01min\u001b[39;00m gold_gains.items() \u001b[38;5;28;01mif\u001b[39;00m s > \u001b[32m0\u001b[39m}\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# üÜï Ranking com TFIDFRetriever\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m ranked, _ = \u001b[43mrank_with_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m metrics[\u001b[33m\"\u001b[39m\u001b[33mMRR@10\u001b[39m\u001b[33m\"\u001b[39m].append(mrr_at_k(ranked, gold_set, k=\u001b[32m10\u001b[39m))\n\u001b[32m     38\u001b[39m metrics[\u001b[33m\"\u001b[39m\u001b[33mnDCG@10\u001b[39m\u001b[33m\"\u001b[39m].append(ndcg_at_k(ranked, gold_gains, k=\u001b[32m10\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mrank_with_tfidf\u001b[39m\u001b[34m(retriever, query_text, topk)\u001b[39m\n\u001b[32m     32\u001b[39m query = Query(query_id=\u001b[33m\"\u001b[39m\u001b[33mtmp\u001b[39m\u001b[33m\"\u001b[39m, text=query_text)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Recupera top-k\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m results = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Extrai doc_ids e scores\u001b[39;00m\n\u001b[32m     38\u001b[39m ranked_items = results.get(\u001b[33m\"\u001b[39m\u001b[33mtmp\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hybrid-retrieval/src/retrievers/tfidf_faiss.py:99\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(self, queries, k)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/hybrid-retrieval/.venv/lib/python3.12/site-packages/faiss/class_wrappers.py:349\u001b[39m, in \u001b[36mhandle_Index.<locals>.replacement_search\u001b[39m\u001b[34m(self, x, k, params, D, I, numeric_type)\u001b[39m\n\u001b[32m    347\u001b[39m n, d = x.shape\n\u001b[32m    348\u001b[39m x = np.ascontiguousarray(x, _numeric_to_str(numeric_type))\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m d == \u001b[38;5;28mself\u001b[39m.d\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m k > \u001b[32m0\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "for ds in DATASETS:\n",
    "    all_results[ds] = evaluate_dataset(ds, only_test=True)\n",
    "\n",
    "print(\"\\n== Resumo (m√©dias por dataset) ==\")\n",
    "display(pd.DataFrame(all_results).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18afdf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scifact   | corpus=  5183 | queries_total= 1109 | qrels_test_linhas=   339 | qrels_test_queries_unicas=  300\n",
      "fiqa      | corpus= 57638 | queries_total= 6648 | qrels_test_linhas=  1706 | qrels_test_queries_unicas=  648\n",
      "nfcorpus  | corpus=  3633 | queries_total= 3237 | qrels_test_linhas= 12334 | qrels_test_queries_unicas=  323\n"
     ]
    }
   ],
   "source": [
    "for name in DATASETS:\n",
    "    df_corpus, df_queries, df_qrels = load_beir_processed(name)\n",
    "    qrels_test = df_qrels[df_qrels[\"split\"] == \"test\"]\n",
    "    print(\n",
    "        f\"{name:9s} | corpus={len(df_corpus):6d} | queries_total={len(df_queries):5d} \"\n",
    "        f\"| qrels_test_linhas={len(qrels_test):6d} | qrels_test_queries_unicas={qrels_test['query_id'].nunique():5d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7918a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
