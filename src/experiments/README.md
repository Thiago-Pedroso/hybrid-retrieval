# üß™ Experiments

Este m√≥dulo cont√©m o **ExperimentRunner** que executa experimentos completos de retrieval a partir de configura√ß√µes YAML/JSON.

---

## üìã **Vis√£o Geral**

O `ExperimentRunner` orquestra o pipeline completo:

1. **Carrega configura√ß√£o**: YAML/JSON ‚Üí `ExperimentConfig` validado
2. **Carrega datasets**: BEIR format ‚Üí corpus, queries, qrels
3. **Executa retrievers**: Para cada retriever, constr√≥i √≠ndice e busca
4. **Avalia resultados**: Calcula m√©tricas (nDCG, MRR, MAP, etc.)
5. **Salva resultados**: CSV, JSON, JSONL

**Fluxo**:
```
Config YAML ‚Üí ExperimentRunner ‚Üí Resultados CSV/JSON
```

---

## üéØ **Componentes**

### **ExperimentRunner**

Runner principal que executa experimentos.

```python
from src.experiments.runner import ExperimentRunner
from src.config.loader import load_config

# Carregar configura√ß√£o
config = load_config("configs/my_experiment.yaml")

# Criar runner
runner = ExperimentRunner(config)

# Executar experimento
results_df = runner.run()
# Retorna: DataFrame com m√©tricas por retriever, dataset, k
```

**Output do DataFrame**:
- Colunas: `k`, `retriever`, `retriever_type`, `dataset`, `split`, `nDCG`, `MRR`, `MAP`, `Recall`, `Precision`, `t_retrieve_sec`
- Uma linha por combina√ß√£o (retriever √ó dataset √ó k)

---

### **run_experiment()**

Fun√ß√£o helper para executar experimento diretamente.

```python
from src.experiments.runner import run_experiment

# Executar experimento
results_df = run_experiment("configs/my_experiment.yaml")

# Salvar manualmente (se necess√°rio)
results_df.to_csv("outputs/results.csv", index=False)
```

---

## üìù **Exemplo de Uso**

### **Configura√ß√£o YAML**

```yaml
# configs/my_experiment.yaml
experiment:
  name: "my_experiment"
  output_dir: "./outputs/experiments/my_experiment"

dataset:
  name: "scifact"
  root: "./data/scifact/processed/beir"
  split_preference: ["test", "dev", "validation", "train"]

retrievers:
  - name: "dense_minilm"
    type: "dense"
    vectorizer:
      type: "dense"
      semantic:
        model: "sentence-transformers/all-MiniLM-L6-v2"
    index:
      type: "faiss"
      metric: "ip"

  - name: "trimodal_heuristic"
    type: "hybrid"
    vectorizer:
      type: "tri_modal"
      semantic:
        model: "sentence-transformers/all-MiniLM-L6-v2"
      tfidf:
        dim: 1000
      graph:
        model: "BAAI/bge-large-en-v1.5"
    fusion:
      strategy: "weighted_cosine"
      policy: "heuristic"
    reranker:
      type: "tri_modal"
      topk_first: 150
    index:
      type: "faiss"
      metric: "ip"

metrics:
  - "nDCG"
  - "MRR"
  - "MAP"
  - "Recall"
  - "Precision"

ks: [1, 3, 5, 10]

output_formats:
  - "csv"
  - "json"
```

### **Executar Experimento**

```python
from src.experiments.runner import run_experiment

# Executar
results_df = run_experiment("configs/my_experiment.yaml")

# Ver resultados
print(results_df.head())

# Filtrar por retriever
dense_results = results_df[results_df["retriever"] == "dense_minilm"]
print(dense_results[["k", "nDCG", "MRR"]])
```

### **M√∫ltiplos Datasets**

```yaml
# configs/multi_dataset.yaml
experiment:
  name: "multi_dataset"
  output_dir: "./outputs/experiments/multi_dataset"

datasets:
  - name: "scifact"
    root: "./data/scifact/processed/beir"
  - name: "fiqa"
    root: "./data/fiqa/processed/beir"
  - name: "nfcorpus"
    root: "./data/nfcorpus/processed/beir"

retrievers:
  - name: "trimodal"
    type: "hybrid"
    # ... configura√ß√£o ...

metrics: ["nDCG", "MRR"]
ks: [1, 5, 10]
```

---

## ‚úÖ **Boas Pr√°ticas**

### **1. Use Nomes Descritivos para Retrievers**

```yaml
# ‚úÖ BOM - Nome descritivo
retrievers:
  - name: "dense_minilm_l6_v2"
    type: "dense"
    # ...

# ‚ùå RUIM - Nome gen√©rico
retrievers:
  - name: "retriever1"
    type: "dense"
    # ...
```

### **2. Organize Outputs por Experimento**

```yaml
# ‚úÖ BOM - Output organizado
experiment:
  name: "scifact_baseline_vs_hybrid"
  output_dir: "./outputs/experiments/scifact_comparison"

# ‚ùå RUIM - Output gen√©rico
experiment:
  name: "test"
  output_dir: "./outputs"
```

### **3. Use Split Preference Apropriada**

```yaml
# ‚úÖ BOM - Prefer√™ncia clara
dataset:
  split_preference: ["test", "dev", "validation", "train"]
  # Tenta test primeiro, depois dev, etc.

# ‚ùå RUIM - Sem prefer√™ncia (pode usar split errado)
dataset:
  # Sem split_preference
```

### **4. Inclua M√©tricas Relevantes**

```yaml
# ‚úÖ BOM - M√©tricas completas
metrics:
  - "nDCG"  # Principal
  - "MRR"   # Ranking
  - "MAP"   # Precis√£o m√©dia
  - "Recall"  # Cobertura
  - "Precision"  # Precis√£o

# ‚ö†Ô∏è OK - M√©tricas m√≠nimas (se performance for cr√≠tica)
metrics:
  - "nDCG"
  - "MRR"
```

### **5. Teste com k Pequeno Primeiro**

```yaml
# ‚úÖ BOM - Testa com k pequeno primeiro
ks: [1, 3, 5, 10]  # Come√ßa pequeno

# ‚ùå RUIM - k muito grande logo de cara
ks: [1, 10, 50, 100]  # 100 pode ser lento demais
```

---

## üîç **Tratamento de Erros**

O `ExperimentRunner` trata erros graciosamente:

- **Dataset n√£o encontrado**: Adiciona linhas de erro no DataFrame
- **Erro ao carregar dataset**: Loga erro e continua com pr√≥ximo dataset
- **Erro ao executar retriever**: Loga erro e adiciona linhas de erro no DataFrame

**Exemplo de output com erro**:
```python
# DataFrame inclui coluna "error" quando h√° problemas
error_rows = results_df[results_df["error"].notna()]
print(error_rows[["retriever", "dataset", "error"]])
```

---

## üìä **An√°lise de Resultados**

### **Agregar por Retriever**

```python
import pandas as pd

# M√©dia de nDCG@10 por retriever
summary = results_df[results_df["k"] == 10].groupby("retriever")["nDCG"].mean()
print(summary.sort_values(ascending=False))
```

### **Comparar Retrievers**

```python
# Comparar dois retrievers
ret1 = results_df[(results_df["retriever"] == "dense_minilm") & (results_df["k"] == 10)]
ret2 = results_df[(results_df["retriever"] == "trimodal") & (results_df["k"] == 10)]

print(f"Dense nDCG@10: {ret1['nDCG'].mean():.4f}")
print(f"TriModal nDCG@10: {ret2['nDCG'].mean():.4f}")
```

### **Exportar para Excel**

```python
# Salvar em Excel (se necess√°rio)
results_df.to_excel("outputs/results.xlsx", index=False)
```

---

## üöÄ **Executar via CLI**

```bash
# Via script
python scripts/run_experiment.py --config configs/my_experiment.yaml

# Direto via Python
python -c "from src.experiments.runner import run_experiment; run_experiment('configs/my_experiment.yaml')"
```

---

## üìö **Refer√™ncias**

- Veja `src/config/README.md` para configura√ß√£o
- Veja `src/eval/README.md` para m√©tricas
- Veja `src/datasets/README.md` para datasets

---

**√öltima atualiza√ß√£o**: 2024

