{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c900cb8",
   "metadata": {},
   "source": [
    "# üî¨ Lab: Valida√ß√£o dos Retrievers Individuais\n",
    "\n",
    "Este notebook demonstra **passo a passo** como funcionam os tr√™s tipos de retrieval implementados no projeto, com √™nfase especial no **retriever de grafo** (entidades + embeddings).\n",
    "\n",
    "## Objetivos\n",
    "1. **TF-IDF Retriever**: Entender retrieval baseado em sobreposi√ß√£o lexical\n",
    "2. **Dense Retriever**: Compreender embeddings sem√¢nticos (MiniLM vs BGE)\n",
    "3. **Graph Retriever**: Explorar em profundidade a extra√ß√£o de entidades, IDF e agrega√ß√£o ponderada\n",
    "\n",
    "Usaremos um mini-corpus de 5 documentos e 3 queries para valida√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0b610",
   "metadata": {},
   "source": [
    "## üì¶ Setup: Imports e Configura√ß√£o do Ambiente\n",
    "\n",
    "Primeiro, garantimos que o Python encontra o reposit√≥rio e importamos as depend√™ncias necess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defff65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/thiago/Documents/GitHub/hybrid-retrieval\n"
     ]
    }
   ],
   "source": [
    "# Se estiver em um ambiente limpo, descomente conforme necess√°rio:\n",
    "# %pip install -U sentence-transformers transformers torch scikit-learn spacy scispacy\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "while repo_root != repo_root.parent and repo_root.name != \"hybrid-retrieval\":\n",
    "    repo_root = repo_root.parent\n",
    "repo_root_str = str(repo_root)\n",
    "if repo_root_str not in sys.path:\n",
    "    sys.path.insert(0, repo_root_str)\n",
    "\n",
    "print(\"Repo root:\", repo_root)\n",
    "\n",
    "# Imports dos schemas do projeto\n",
    "from src.datasets.schema import Document, Query\n",
    "\n",
    "# Silencia warnings de modelos\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb14b44",
   "metadata": {},
   "source": [
    "## üìö Cria√ß√£o do Mini-Corpus\n",
    "\n",
    "Criamos 5 documentos com temas variados para testar os diferentes comportamentos dos retrievers:\n",
    "- **D1**: COVID-19 e vacinas (dom√≠nio m√©dico, entidades: \"COVID-19\", \"mRNA vaccines\")\n",
    "- **D2**: Mercados financeiros (dom√≠nio financeiro, entidades: \"equity market\", \"US economy\")\n",
    "- **D3**: Graph Neural Networks (dom√≠nio t√©cnico, entidades: \"GNNs\", \"entity classification\")\n",
    "- **D4**: Nutri√ß√£o (dom√≠nio sa√∫de, entidades: \"apples\", \"fiber\", \"vitamins\")\n",
    "- **D5**: Large Language Models (dom√≠nio ML, entidades: \"MiniLM\", \"BGE\")\n",
    "\n",
    "E 3 queries que exploram diferentes aspectos:\n",
    "- **Q1**: Busca sobre efic√°cia de vacinas (termos relacionados a D1)\n",
    "- **Q2**: Busca sobre impacto de taxas de juros (relacionada a D2)\n",
    "- **Q3**: Busca sobre embeddings de entidades e grafos (relacionada a D3 e D5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6346c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Corpus: 5 documentos\n",
      "‚úì Queries: 3 consultas\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        doc_id=\"D1\", \n",
    "        title=\"COVID-19 vaccines\", \n",
    "        text=\"Efficacy and side effects of mRNA COVID-19 vaccines in clinical trials.\"\n",
    "    ),\n",
    "    Document(\n",
    "        doc_id=\"D2\", \n",
    "        title=\"Financial markets overview\", \n",
    "        text=\"Equity market volatility and interest rates in the US economy during 2023.\"\n",
    "    ),\n",
    "    Document(\n",
    "        doc_id=\"D3\", \n",
    "        title=\"Graph neural networks\", \n",
    "        text=\"Using GNNs for entity classification and link prediction tasks in knowledge graphs.\"\n",
    "    ),\n",
    "    Document(\n",
    "        doc_id=\"D4\", \n",
    "        title=\"Nutritional benefits of apples\", \n",
    "        text=\"Apples contain dietary fiber and vitamins, reducing cardiovascular health risks.\"\n",
    "    ),\n",
    "    Document(\n",
    "        doc_id=\"D5\", \n",
    "        title=\"Large language models for retrieval\", \n",
    "        text=\"MiniLM and BGE embeddings improve semantic retrieval and reranking performance.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    Query(query_id=\"Q1\", text=\"Are mRNA vaccines effective against COVID?\"),\n",
    "    Query(query_id=\"Q2\", text=\"How do interest rates impact equity volatility?\"),\n",
    "    Query(query_id=\"Q3\", text=\"Entity embeddings and graph-based retrieval methods\"),\n",
    "]\n",
    "\n",
    "print(f\"‚úì Corpus: {len(docs)} documentos\")\n",
    "print(f\"‚úì Queries: {len(queries)} consultas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef899f",
   "metadata": {},
   "source": [
    "---\n",
    "# üî§ Parte 1: TF-IDF Retriever (Sinal Lexical)\n",
    "\n",
    "## O que √© TF-IDF?\n",
    "- **Term Frequency (TF)**: Frequ√™ncia do termo no documento\n",
    "- **Inverse Document Frequency (IDF)**: Penaliza termos comuns no corpus\n",
    "- **Vetor TF-IDF**: Cada documento √© representado como um vetor esparso de tamanho = vocabul√°rio\n",
    "\n",
    "## Como funciona?\n",
    "1. **Fit**: Constr√≥i vocabul√°rio do corpus e calcula IDF para cada termo\n",
    "2. **Transform**: Converte documento ‚Üí vetor TF-IDF normalizado (L2)\n",
    "3. **Busca**: Similaridade por inner-product (equivalente a cosseno ap√≥s normaliza√ß√£o)\n",
    "\n",
    "## Vantagens\n",
    "‚úì R√°pido e interpret√°vel  \n",
    "‚úì Bom para matching exato de termos  \n",
    "\n",
    "## Limita√ß√µes\n",
    "‚úó N√£o captura sin√¥nimos ou par√°frases  \n",
    "‚úó Sens√≠vel a varia√ß√µes morfol√≥gicas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bfc65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Construindo √≠ndice TF-IDF...\n",
      "2025-10-29 18:07:11 | INFO     | retriever.tfidf | [tfidf_faiss.py:66] | üöÄ Building TF-IDF Index (5 documentos)\n",
      "2025-10-29 18:07:11 | INFO     | retriever.tfidf | [logging.py:199] | ‚è±Ô∏è  Fit TF-IDF no corpus - iniciando...\n",
      "2025-10-29 18:07:11 | INFO     | tfidf.vectorizer | [logging.py:199] | ‚è±Ô∏è  Fit TF-IDF - iniciando...\n",
      "2025-10-29 18:07:12 | INFO     | tfidf.vectorizer | [logging.py:220] | ‚úì Fit TF-IDF - conclu√≠do em \u001b[32m1.21s\u001b[0m\n",
      "2025-10-29 18:07:12 | INFO     | tfidf.vectorizer | [tfidf_vectorizer.py:19] | ‚úì TF-IDF fitted: vocab_size=60\n",
      "2025-10-29 18:07:12 | INFO     | retriever.tfidf | [logging.py:220] | ‚úì Fit TF-IDF no corpus - conclu√≠do em \u001b[32m1.21s\u001b[0m\n",
      "2025-10-29 18:07:12 | INFO     | retriever.tfidf | [logging.py:199] | ‚è±Ô∏è  Encoding documents (TF-IDF) - iniciando...\n",
      "2025-10-29 18:07:12 | INFO     | retriever.tfidf | [logging.py:220] | ‚úì Encoding documents (TF-IDF) - conclu√≠do em \u001b[32m1.0ms\u001b[0m\n",
      "2025-10-29 18:07:12 | INFO     | retriever.tfidf | [logging.py:199] | ‚è±Ô∏è  Construindo FAISS IndexFlatIP - iniciando...\n",
      "2025-10-29 18:07:12 | INFO     | retriever.tfidf | [logging.py:220] | ‚úì Construindo FAISS IndexFlatIP - conclu√≠do em \u001b[32m0.6ms\u001b[0m\n",
      "2025-10-29 18:07:12 | INFO     | retriever.tfidf | [tfidf_faiss.py:85] |   ‚úì FAISS IndexFlatIP: 5 vetores, dim=1000\n",
      "\n",
      "üîç Executando retrieval (top-3 por query)...\n",
      "\n",
      "üìä Query [Q1]: Are mRNA vaccines effective against COVID?\n",
      "  1. D1 (score=0.6566) - COVID-19 vaccines\n",
      "  2. D3 (score=0.0000) - Graph neural networks\n",
      "  3. D2 (score=0.0000) - Financial markets overview\n",
      "\n",
      "üìä Query [Q2]: How do interest rates impact equity volatility?\n",
      "  1. D2 (score=0.5408) - Financial markets overview\n",
      "  2. D3 (score=0.0000) - Graph neural networks\n",
      "  3. D1 (score=0.0000) - COVID-19 vaccines\n",
      "\n",
      "üìä Query [Q3]: Entity embeddings and graph-based retrieval methods\n",
      "  1. D5 (score=0.4259) - Large language models for retrieval\n",
      "  2. D3 (score=0.2967) - Graph neural networks\n",
      "  3. D2 (score=0.0299) - Financial markets overview\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.retrievers.tfidf_faiss import TFIDFRetriever\n",
    "\n",
    "tfidf_r = TFIDFRetriever(\n",
    "    dim=1000,              # tamanho m√°ximo do vocabul√°rio\n",
    "    min_df=1,              # aceita termos que aparecem pelo menos 1x\n",
    "    backend=\"sklearn\",     # usa scikit-learn TfidfVectorizer\n",
    "    use_faiss=True,        # IndexFlatIP para busca r√°pida\n",
    "    artifact_dir=None,     # sem persist√™ncia (apenas para lab)\n",
    "    index_name=\"tfidf.index\",\n",
    ")\n",
    "\n",
    "print(\"üîß Construindo √≠ndice TF-IDF...\")\n",
    "tfidf_r.build_index(docs)\n",
    "\n",
    "print(\"\\nüîç Executando retrieval (top-3 por query)...\")\n",
    "tfidf_results = tfidf_r.retrieve(queries, k=3)\n",
    "\n",
    "# Exibe resultados\n",
    "for q in queries:\n",
    "    print(f\"\\nüìä Query [{q.query_id}]: {q.text}\")\n",
    "    pairs = tfidf_results.get(q.query_id, [])\n",
    "    for rank, (doc_id, score) in enumerate(pairs, 1):\n",
    "        doc = next(d for d in docs if d.doc_id == doc_id)\n",
    "        print(f\"  {rank}. {doc_id} (score={score:.4f}) - {doc.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a7b88",
   "metadata": {},
   "source": [
    "### üìà An√°lise dos Resultados TF-IDF\n",
    "\n",
    "**Observe**:\n",
    "- Queries com **termos exatos** do corpus (ex: \"mRNA\", \"vaccines\", \"COVID\") ranqueiam bem documentos correspondentes\n",
    "- Queries com **par√°frases** (ex: \"effective\" vs \"efficacy\") podem ter scores mais baixos\n",
    "- O TF-IDF √© **lexical**: n√£o entende que \"interest rates\" e \"taxas de juros\" s√£o sin√¥nimos\n",
    "\n",
    "**Exemplo**: Q1 ranquea D1 no topo, pois compartilha \"mRNA\", \"vaccines\", \"COVID\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383d958",
   "metadata": {},
   "source": [
    "---\n",
    "# üß† Parte 2: Dense Retriever (Embeddings Sem√¢nticos)\n",
    "\n",
    "## O que √© Dense Retrieval?\n",
    "- Usa **modelos de linguagem** (ex: MiniLM, BGE) para gerar embeddings densos\n",
    "- Cada documento/query ‚Üí vetor cont√≠nuo de 384d (MiniLM) ou 1024d (BGE-Large)\n",
    "- Similaridade captura **significado sem√¢ntico**, n√£o apenas palavras exatas\n",
    "\n",
    "## Modelos comparados no paper\n",
    "| Modelo | Dimens√£o | Par√¢metros | Uso |\n",
    "|--------|----------|------------|-----|\n",
    "| **MiniLM-L6-v2** | 384 | 22M | R√°pido, eficiente |\n",
    "| **BGE-Large** | 1024 | 335M | Maior qualidade |\n",
    "\n",
    "## Por que MiniLM pode superar BGE no h√≠brido?\n",
    "O paper mostra que embeddings menores podem ter **melhor alinhamento com LLMs** durante o reranking (fen√¥meno chamado \"FAISS Hybrid Paradox\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a85cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thiago/Documents/GitHub/hybrid-retrieval/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Construindo √≠ndice denso com sentence-transformers/all-MiniLM-L6-v2...\n",
      "   Dimens√£o: 384d\n",
      "2025-10-29 18:07:15 | INFO     | retriever.dense | [dense_faiss.py:75] | üöÄ Building Dense Index (5 documentos)\n",
      "2025-10-29 18:07:15 | INFO     | retriever.dense | [logging.py:199] | ‚è±Ô∏è  Encoding documents - iniciando...\n"
     ]
    }
   ],
   "source": [
    "from src.retrievers.dense_faiss import DenseFaiss\n",
    "\n",
    "# Escolha do modelo (altere para testar)\n",
    "dense_model = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "# Alternativa: \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "dense_r = DenseFaiss(\n",
    "    model_name=dense_model,\n",
    "    device=None,              # Use \"cuda:0\" se tiver GPU dispon√≠vel\n",
    "    query_prefix=\"\",\n",
    "    doc_prefix=\"\",\n",
    "    use_faiss=True,\n",
    "    artifact_dir=None,\n",
    "    index_name=\"dense.index\",\n",
    ")\n",
    "\n",
    "print(f\"üîß Construindo √≠ndice denso com {dense_model}...\")\n",
    "print(f\"   Dimens√£o: {dense_r.dim}d\")\n",
    "dense_r.build_index(docs)\n",
    "\n",
    "print(\"\\nüîç Executando retrieval (top-3 por query)...\")\n",
    "dense_results = dense_r.retrieve(queries, k=3)\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nüìä Query [{q.query_id}]: {q.text}\")\n",
    "    pairs = dense_results.get(q.query_id, [])\n",
    "    for rank, (doc_id, score) in enumerate(pairs, 1):\n",
    "        doc = next(d for d in docs if d.doc_id == doc_id)\n",
    "        print(f\"  {rank}. {doc_id} (score={score:.4f}) - {doc.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef66b78",
   "metadata": {},
   "source": [
    "### üìà An√°lise dos Resultados Dense\n",
    "\n",
    "**Observe**:\n",
    "- Melhor captura de **sin√¥nimos** e **par√°frases** (ex: \"effective\" ‚âà \"efficacy\")\n",
    "- Scores geralmente mais **uniformes** do que TF-IDF (espa√ßo denso √© mais suave)\n",
    "- Pode ranquear documentos **tematicamente relacionados** mesmo sem overlap lexical\n",
    "\n",
    "**Experimento**: Troque para `\"BAAI/bge-large-en-v1.5\"` e compare os resultados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82881cd",
   "metadata": {},
   "source": [
    "---\n",
    "# üï∏Ô∏è Parte 3: Graph Retriever (Entidades + Embeddings)\n",
    "\n",
    "## Vis√£o Geral\n",
    "O retriever de grafo implementa o **terceiro modo (g)** do paper:\n",
    "1. **Extra√ß√£o de Entidades**: Identifica \"nomes\" importantes no texto (pessoas, locais, termos t√©cnicos)\n",
    "2. **IDF de Entidades**: Calcula import√¢ncia relativa de cada entidade no corpus\n",
    "3. **Embedding de Entidades**: Cada entidade ‚Üí vetor denso (ex: BGE-Large 1024d)\n",
    "4. **Agrega√ß√£o TF-IDF**: Vetor do documento = Œ£ [TF(e) √ó IDF(e) √ó emb(e)]\n",
    "5. **Normaliza√ß√£o L2**: Vetor final √© normalizado para compara√ß√£o por cosseno\n",
    "\n",
    "## F√≥rmula\n",
    "\n",
    "g(text) = L2_norm( Œ£_{e ‚àà entities(text)} TF(e, text) √ó IDF(e) √ó embedding(e) )\n",
    "\n",
    "\n",
    "## Por que √© √∫til?\n",
    "- Captura **sinal estruturado** de entidades nomeadas\n",
    "- √ötil em dom√≠nios com **terminologia espec√≠fica** (medicina, finan√ßas, produtos)\n",
    "- Complementa sinais denso e lexical no h√≠brido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7a524",
   "metadata": {},
   "source": [
    "## üîç Passo 1: Extra√ß√£o de Entidades\n",
    "\n",
    "Vamos explorar **manualmente** como as entidades s√£o extra√≠das de um documento exemplo antes de usar o retriever completo.\n",
    "\n",
    "### M√©todos de NER dispon√≠veis:\n",
    "1. **sciSpaCy** (recomendado para textos cient√≠ficos): `en_ner_bc5cdr_md`, `en_core_sci_md`\n",
    "2. **spaCy padr√£o**: `en_core_web_sm`, `en_core_web_md`\n",
    "3. **Fallback regex** (quando spaCy n√£o est√° instalado): detecta palavras capitalizadas e termos longos\n",
    "\n",
    "### O que √© extra√≠do?\n",
    "- **Entidades nomeadas**: detectadas pelo modelo NER (ex: DISEASE, CHEMICAL, ORG)\n",
    "- **Noun chunks** (opcional): sintagmas nominais (ex: \"mRNA vaccines\", \"clinical trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos inspecionar a extra√ß√£o de entidades no documento D1 manualmente\n",
    "\n",
    "doc_text = docs[0].title + \" \" + docs[0].text\n",
    "print(f\"üìÑ Documento D1:\\n{doc_text}\\n\")\n",
    "\n",
    "# Tentamos carregar spaCy (pode falhar se n√£o instalado)\n",
    "try:\n",
    "    import spacy\n",
    "    # Tenta sciSpaCy primeiro (melhor para textos cient√≠ficos)\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_ner_bc5cdr_md\", disable=[\"tagger\", \"parser\", \"lemmatizer\"])\n",
    "        print(\"‚úì Usando sciSpaCy: en_ner_bc5cdr_md\")\n",
    "    except:\n",
    "        # Fallback para spaCy padr√£o\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"lemmatizer\"])\n",
    "        print(\"‚úì Usando spaCy: en_core_web_sm\")\n",
    "    \n",
    "    doc = nlp(doc_text)\n",
    "    \n",
    "    # Extrai entidades nomeadas\n",
    "    entities = []\n",
    "    print(\"\\nüè∑Ô∏è  Entidades Nomeadas Detectadas:\")\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text.lower().strip())\n",
    "        print(f\"   - '{ent.text}' [{ent.label_}]\")\n",
    "    \n",
    "    # Extrai noun chunks\n",
    "    print(\"\\nüì¶ Noun Chunks Detectados:\")\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        if chunk_text not in entities:\n",
    "            entities.append(chunk_text)\n",
    "        print(f\"   - '{chunk.text}'\")\n",
    "    \n",
    "    print(f\"\\n‚úì Total de entidades √∫nicas: {len(set(entities))}\")\n",
    "    print(f\"  Entidades: {sorted(set(entities))}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  spaCy n√£o dispon√≠vel: {e}\")\n",
    "    print(\"   Usando fallback regex (detecta termos capitalizados e longos)\")\n",
    "    \n",
    "    # Fallback simples: palavras capitalizadas ou termos longos\n",
    "    import re\n",
    "    pattern = re.compile(r\"[A-Za-z][A-Za-z0-9_\\-/\\.]{1,}\")\n",
    "    candidates = []\n",
    "    for tok in pattern.findall(doc_text):\n",
    "        if len(tok) >= 10 or tok[0].isupper():\n",
    "            candidates.append(tok.lower())\n",
    "    print(f\"\\n  Entidades detectadas (fallback): {sorted(set(candidates))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0aa3a",
   "metadata": {},
   "source": [
    "### üí° Interpreta√ß√£o da Extra√ß√£o\n",
    "\n",
    "**Entidades nomeadas** capturam:\n",
    "- Nomes pr√≥prios: \"COVID-19\", \"US\", \"MiniLM\"\n",
    "- Termos t√©cnicos: \"mRNA\", \"GNNs\", \"equity market\"\n",
    "- Conceitos importantes: \"vaccines\", \"clinical trials\"\n",
    "\n",
    "**Noun chunks** adicionam:\n",
    "- Sintagmas nominais completos: \"mRNA vaccines\", \"interest rates\"\n",
    "- Contexto adicional al√©m de palavras isoladas\n",
    "\n",
    "**Normaliza√ß√£o aplicada**:\n",
    "- Lowercase para uniformidade\n",
    "- Remo√ß√£o de pontua√ß√£o nas bordas\n",
    "- M√≠nimo 2 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4650d",
   "metadata": {},
   "source": [
    "## üßÆ Passo 2: C√°lculo do IDF de Entidades\n",
    "\n",
    "Ap√≥s extrair entidades de **todos** os documentos, calculamos o IDF:\n",
    "\n",
    "IDF(e) = log((1 + N) / (1 + DF(e))) + 1.0\n",
    "\n",
    "\n",
    "Onde:\n",
    "- **N** = n√∫mero total de documentos\n",
    "- **DF(e)** = n√∫mero de documentos que cont√™m a entidade `e`\n",
    "\n",
    "**Intui√ß√£o**: Entidades raras (baixo DF) recebem maior peso; entidades comuns (alto DF) s√£o penalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6af267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos calcular manualmente o IDF das entidades no nosso corpus\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "print(\"üî¢ Calculando IDF de Entidades no Corpus\\n\")\n",
    "\n",
    "# Fun√ß√£o simplificada de extra√ß√£o (fallback regex)\n",
    "def extract_entities_simple(text):\n",
    "    \"\"\"Extrai entidades via regex (fallback)\"\"\"\n",
    "    pattern = re.compile(r\"[A-Za-z][A-Za-z0-9_\\-/\\.]{1,}\")\n",
    "    entities = []\n",
    "    for tok in pattern.findall(text):\n",
    "        if len(tok) >= 10 or tok[0].isupper():\n",
    "            entities.append(tok.lower().strip(\".,;:()[]{}\"))\n",
    "    return [e for e in entities if len(e) >= 2]\n",
    "\n",
    "# Conta DF (document frequency) de cada entidade\n",
    "df = defaultdict(int)\n",
    "N = len(docs)\n",
    "\n",
    "for doc in docs:\n",
    "    text = (doc.title or \"\") + \" \" + (doc.text or \"\")\n",
    "    entities = set(extract_entities_simple(text))\n",
    "    for e in entities:\n",
    "        df[e] += 1\n",
    "\n",
    "# Calcula IDF\n",
    "entity_idf = {}\n",
    "for e, count in df.items():\n",
    "    idf_value = np.log((1 + N) / (1 + count)) + 1.0\n",
    "    entity_idf[e] = idf_value\n",
    "\n",
    "# Ordena por IDF decrescente\n",
    "sorted_entities = sorted(entity_idf.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"üìä Top 10 Entidades por IDF (mais raras = maior IDF):\\n\")\n",
    "for e, idf in sorted_entities[:10]:\n",
    "    print(f\"   {e:<20} | DF={df[e]:>2} | IDF={idf:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úì Total de entidades √∫nicas no corpus: {len(entity_idf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745656e",
   "metadata": {},
   "source": [
    "### üìà Interpreta√ß√£o do IDF\n",
    "\n",
    "**Entidades com IDF alto** (raras):\n",
    "- Aparecem em poucos documentos\n",
    "- S√£o mais **discriminativas** (ajudam a diferenciar documentos)\n",
    "- Exemplo: \"cardiovascular\" (s√≥ em D4), \"gnns\" (s√≥ em D3)\n",
    "\n",
    "**Entidades com IDF baixo** (comuns):\n",
    "- Aparecem em muitos documentos\n",
    "- S√£o menos √∫teis para distinguir conte√∫do\n",
    "- Exemplo: termos gen√©ricos que aparecem em m√∫ltiplos contextos\n",
    "\n",
    "**Por que isso importa?** \n",
    "O IDF pondera a contribui√ß√£o de cada entidade no vetor final, dando mais peso a termos discriminativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0706a2",
   "metadata": {},
   "source": [
    "## üéØ Passo 3: Embedding de Entidades\n",
    "\n",
    "Cada entidade √© transformada em um **vetor denso** usando um modelo HF (ex: BGE-Large).\n",
    "\n",
    "**Exemplo**: \n",
    "- Entidade: `\"mRNA vaccines\"`\n",
    "- Embedding: vetor de 1024 dimens√µes (se BGE-Large)\n",
    "\n",
    "**Por que embeddings de entidades?**\n",
    "- Captura **similaridade sem√¢ntica entre entidades** (ex: \"mRNA\" ‚âà \"RNA-based\")\n",
    "- Permite que entidades relacionadas contribuam de forma similar ao vetor do documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ver como uma entidade √© transformada em embedding\n",
    "\n",
    "from src.encoders.encoders import HFSemanticEncoder, l2norm\n",
    "\n",
    "print(\"üß¨ Gerando Embeddings de Entidades\\n\")\n",
    "\n",
    "# Carrega encoder (mesmo usado no retriever de grafo)\n",
    "entity_encoder = HFSemanticEncoder(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",  # BGE-Large (1024d)\n",
    "    device=None,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Modelo: {entity_encoder.model_name}\")\n",
    "print(f\"‚úì Dimens√£o: {entity_encoder.dim}d\\n\")\n",
    "\n",
    "# Embute algumas entidades exemplo\n",
    "sample_entities = [\"mrna vaccines\", \"covid-19\", \"equity market\", \"gnns\"]\n",
    "\n",
    "print(\"üìä Embeddings de Entidades (primeiras 5 dimens√µes):\\n\")\n",
    "embeddings = {}\n",
    "for ent in sample_entities:\n",
    "    emb = l2norm(entity_encoder.encode_text(ent, is_query=False))\n",
    "    embeddings[ent] = emb\n",
    "    norm = np.linalg.norm(emb)\n",
    "    print(f\"   {ent:<20} ‚Üí [{emb[:5]}...] || norm={norm:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Embeddings gerados e normalizados (L2 norm ‚âà 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba440d5",
   "metadata": {},
   "source": [
    "### üîó Similaridade entre Entidades\n",
    "\n",
    "Vamos calcular a **similaridade de cosseno** entre pares de entidades para verificar se o modelo captura rela√ß√µes sem√¢nticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc222348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula similaridade de cosseno entre pares de entidades\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    return float(np.dot(v1, v2))  # J√° normalizados (L2), ent√£o dot = cosseno\n",
    "\n",
    "print(\"üîó Similaridade entre Entidades (cosseno):\\n\")\n",
    "\n",
    "pairs = [\n",
    "    (\"mrna vaccines\", \"covid-19\"),      # relacionadas (dom√≠nio m√©dico)\n",
    "    (\"equity market\", \"gnns\"),          # n√£o relacionadas (finan√ßas vs ML)\n",
    "    (\"mrna vaccines\", \"equity market\"), # n√£o relacionadas (medicina vs finan√ßas)\n",
    "]\n",
    "\n",
    "for e1, e2 in pairs:\n",
    "    if e1 in embeddings and e2 in embeddings:\n",
    "        sim = cosine_sim(embeddings[e1], embeddings[e2])\n",
    "        print(f\"   '{e1}' ‚Üî '{e2}': {sim:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Entidades semanticamente relacionadas t√™m similaridade maior!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95c7fc",
   "metadata": {},
   "source": [
    "## üß© Passo 4: Agrega√ß√£o TF-IDF de Entidades\n",
    "\n",
    "Agora combinamos tudo para gerar o **vetor do documento**:\n",
    "\n",
    "g(doc) = L2_norm( Œ£_{e ‚àà doc} TF(e, doc) √ó IDF(e) √ó embedding(e) )\n",
    "\n",
    "\n",
    "Onde:\n",
    "- **TF(e, doc)** = frequ√™ncia da entidade no documento\n",
    "- **IDF(e)** = inverse document frequency (calculado no passo 2)\n",
    "- **embedding(e)** = vetor denso da entidade (calculado no passo 3)\n",
    "\n",
    "**Algoritmo**:\n",
    "1. Extrair entidades do documento\n",
    "2. Contar TF de cada entidade\n",
    "3. Para cada entidade: peso = TF √ó IDF\n",
    "4. Somar: vetor_final = Œ£ (peso √ó embedding)\n",
    "5. Normalizar L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos calcular manualmente o vetor de grafo para o documento D1\n",
    "\n",
    "doc_d1 = docs[0]\n",
    "text_d1 = (doc_d1.title or \"\") + \" \" + (doc_d1.text or \"\")\n",
    "print(f\"üìÑ Documento D1:\\n{text_d1}\\n\")\n",
    "\n",
    "# 1. Extrai entidades\n",
    "entities_d1 = extract_entities_simple(text_d1)\n",
    "print(f\"üè∑Ô∏è  Entidades extra√≠das: {entities_d1}\\n\")\n",
    "\n",
    "# 2. Conta TF\n",
    "from collections import Counter\n",
    "tf_d1 = Counter(entities_d1)\n",
    "print(f\"üî¢ Term Frequency (TF):\")\n",
    "for e, count in tf_d1.items():\n",
    "    print(f\"   {e}: {count}\")\n",
    "\n",
    "# 3. Calcula vetor agregado (manual)\n",
    "print(f\"\\nüßÆ Agrega√ß√£o TF-IDF √ó Embeddings:\")\n",
    "vec_d1 = np.zeros(entity_encoder.dim, dtype=np.float32)\n",
    "\n",
    "for e, tf in tf_d1.items():\n",
    "    if e in entity_idf:  # entidade conhecida no corpus\n",
    "        idf = entity_idf[e]\n",
    "        weight = tf * idf\n",
    "        \n",
    "        # Gera embedding da entidade (ou usa cache)\n",
    "        if e not in embeddings:\n",
    "            embeddings[e] = l2norm(entity_encoder.encode_text(e, is_query=False))\n",
    "        emb = embeddings[e]\n",
    "        \n",
    "        vec_d1 += weight * emb\n",
    "        print(f\"   {e:<20} | TF={tf} | IDF={idf:.4f} | weight={weight:.4f}\")\n",
    "\n",
    "# 4. Normaliza L2\n",
    "vec_d1 = l2norm(vec_d1)\n",
    "print(f\"\\n‚úì Vetor final (primeiras 5 dims): {vec_d1[:5]}\")\n",
    "print(f\"‚úì Norma L2: {np.linalg.norm(vec_d1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f708b",
   "metadata": {},
   "source": [
    "### üí° Interpreta√ß√£o da Agrega√ß√£o\n",
    "\n",
    "**Contribui√ß√£o de cada entidade**:\n",
    "- Entidades **frequentes** no documento (alto TF) contribuem mais\n",
    "- Entidades **raras** no corpus (alto IDF) t√™m maior peso relativo\n",
    "- Embedding captura o **significado sem√¢ntico** da entidade\n",
    "\n",
    "**Resultado final**:\n",
    "- Vetor denso de 1024d (se BGE-Large) normalizado\n",
    "- Representa o documento como uma **composi√ß√£o ponderada de suas entidades**\n",
    "- Permite compara√ß√£o por cosseno com queries que tamb√©m passam pelo mesmo processo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6965fd",
   "metadata": {},
   "source": [
    "## üöÄ Retriever de Grafo Completo\n",
    "\n",
    "Agora que entendemos o passo a passo, vamos usar o **GraphRetriever** oficial que automatiza todo esse pipeline:\n",
    "\n",
    "1. **Fit**: Extrai entidades de todos os documentos e calcula IDF\n",
    "2. **Build**: Gera embeddings e vetores agregados, indexa no FAISS\n",
    "3. **Retrieve**: Para cada query, gera vetor e busca top-K por cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5804c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retrievers.graph_faiss import GraphRetriever\n",
    "\n",
    "graph_r = GraphRetriever(\n",
    "    graph_model_name=\"BAAI/bge-large-en-v1.5\",  # BGE-Large para embeddings de entidades\n",
    "    device=None,                                # \"cuda:0\" para GPU\n",
    "    ner_backend=\"scispacy\",                     # ou \"spacy\" / \"none\" (fallback)\n",
    "    ner_model=None,                             # auto-detecta modelo dispon√≠vel\n",
    "    ner_use_noun_chunks=True,                   # inclui noun chunks\n",
    "    ner_batch_size=64,\n",
    "    ner_n_process=1,\n",
    "    ner_allowed_labels=None,                    # aceita todas as labels NER\n",
    "    min_df=1,                                   # min document frequency\n",
    "    use_faiss=True,\n",
    "    artifact_dir=None,\n",
    "    index_name=\"graph.index\",\n",
    "    entity_artifact_dir=None,                   # sem cache neste lab\n",
    "    entity_force_rebuild=False,\n",
    ")\n",
    "\n",
    "print(\"üîß Construindo √≠ndice de grafo...\")\n",
    "print(f\"   Modelo: {graph_r.vec.encoder.model_name}\")\n",
    "print(f\"   Dimens√£o: {graph_r.vec.dim}d\")\n",
    "graph_r.build_index(docs)\n",
    "\n",
    "print(\"\\nüîç Executando retrieval (top-3 por query)...\")\n",
    "graph_results = graph_r.retrieve(queries, k=3)\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nüìä Query [{q.query_id}]: {q.text}\")\n",
    "    pairs = graph_results.get(q.query_id, [])\n",
    "    for rank, (doc_id, score) in enumerate(pairs, 1):\n",
    "        doc = next(d for d in docs if d.doc_id == doc_id)\n",
    "        print(f\"  {rank}. {doc_id} (score={score:.4f}) - {doc.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005b2fe",
   "metadata": {},
   "source": [
    "### üìà An√°lise dos Resultados Graph\n",
    "\n",
    "**Observe**:\n",
    "- Queries com **entidades nomeadas fortes** (ex: \"mRNA\", \"COVID\", \"entity embeddings\") devem ranquear bem documentos relacionados\n",
    "- O retriever de grafo √© especialmente forte quando:\n",
    "  - Documento e query compartilham **entidades raras** (alto IDF)\n",
    "  - Entidades t√™m **alta similaridade sem√¢ntica** nos embeddings\n",
    "- Pode capturar rela√ß√µes que TF-IDF perde (par√°frases de entidades) e que Dense n√£o enfatiza (import√¢ncia de termos espec√≠ficos)\n",
    "\n",
    "**Exemplo esperado**: \n",
    "- Q3 (\"Entity embeddings and graph-based retrieval\") deve ranquear bem D3 (GNNs, entity classification) e D5 (embeddings para retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289a978",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Compara√ß√£o Lado a Lado dos Tr√™s Retrievers\n",
    "\n",
    "Vamos consolidar os resultados dos tr√™s m√©todos para facilitar a an√°lise comparativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e280f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_results(results, name):\n",
    "    \"\"\"Formata resultados em DataFrame para visualiza√ß√£o\"\"\"\n",
    "    rows = []\n",
    "    for q in queries:\n",
    "        pairs = results.get(q.query_id, [])[:3]\n",
    "        for rank, (doc_id, score) in enumerate(pairs, 1):\n",
    "            doc = next(d for d in docs if d.doc_id == doc_id)\n",
    "            rows.append({\n",
    "                'Query': q.query_id,\n",
    "                'Retriever': name,\n",
    "                'Rank': rank,\n",
    "                'Doc': doc_id,\n",
    "                'Score': f\"{score:.4f}\",\n",
    "                'Title': doc.title[:40]\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Concatena resultados\n",
    "df_tfidf = format_results(tfidf_results, \"TF-IDF\")\n",
    "df_dense = format_results(dense_results, \"Dense\")\n",
    "df_graph = format_results(graph_results, \"Graph\")\n",
    "\n",
    "df_all = pd.concat([df_tfidf, df_dense, df_graph], ignore_index=True)\n",
    "\n",
    "print(\"üìä Compara√ß√£o dos Retrievers (Top-3 por Query):\\n\")\n",
    "for qid in [\"Q1\", \"Q2\", \"Q3\"]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query {qid}: {next(q.text for q in queries if q.query_id == qid)}\")\n",
    "    print('='*80)\n",
    "    subset = df_all[df_all['Query'] == qid]\n",
    "    print(subset[['Retriever', 'Rank', 'Doc', 'Score', 'Title']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4a8e4",
   "metadata": {},
   "source": [
    "### üîç An√°lise Comparativa\n",
    "\n",
    "| Retriever | For√ßa | Fraqueza |\n",
    "|-----------|-------|----------|\n",
    "| **TF-IDF** | ‚úì Match exato de termos<br>‚úì R√°pido e interpret√°vel | ‚úó N√£o captura sin√¥nimos<br>‚úó Sens√≠vel a vocabul√°rio |\n",
    "| **Dense** | ‚úì Similaridade sem√¢ntica<br>‚úì Generaliza√ß√£o | ‚úó Pode perder signal lexical espec√≠fico<br>‚úó Menos interpret√°vel |\n",
    "| **Graph** | ‚úì Captura entidades importantes<br>‚úì Pondera√ß√£o por IDF | ‚úó Depende da qualidade do NER<br>‚úó Mais lento (extra√ß√£o + embedding) |\n",
    "\n",
    "**Conclus√£o**: Cada retriever captura um **sinal complementar**. O retriever **h√≠brido** combina os tr√™s para obter o melhor de todos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303901d",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¨ Explora√ß√£o Adicional: Dimens√µes e Normaliza√ß√£o\n",
    "\n",
    "Vamos verificar as dimens√µes e normas L2 dos vetores gerados por cada retriever para confirmar que est√£o corretamente normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb123af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica dimens√µes e normaliza√ß√£o\n",
    "\n",
    "print(\"üìê Dimens√µes e Normaliza√ß√£o dos Vetores\\n\")\n",
    "\n",
    "# TF-IDF\n",
    "from src.vectorizers.tfidf_vectorizer import TFIDFVectorizer\n",
    "tf_vec = TFIDFVectorizer(dim=1000, min_df=1, backend=\"sklearn\")\n",
    "tf_vec.fit_corpus([(d.title or \"\") + \" \" + (d.text or \"\") for d in docs])\n",
    "v_tf = tf_vec.encode_text(queries[0].text)\n",
    "print(f\"TF-IDF:\")\n",
    "print(f\"   Dim: {v_tf.shape[0]:>4}d | L2 norm: {np.linalg.norm(v_tf):.6f} | Sparsity: {np.sum(v_tf == 0) / len(v_tf) * 100:.1f}%\")\n",
    "\n",
    "# Dense (MiniLM)\n",
    "from src.vectorizers.dense_vectorizer import DenseVectorizer\n",
    "dv = DenseVectorizer(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=None)\n",
    "v_d = dv.encode_query(queries[0].text)\n",
    "print(f\"\\nDense (MiniLM):\")\n",
    "print(f\"   Dim: {v_d.shape[0]:>4}d | L2 norm: {np.linalg.norm(v_d):.6f} | Sparsity: {np.sum(v_d == 0) / len(v_d) * 100:.1f}%\")\n",
    "\n",
    "# Graph\n",
    "from src.vectorizers.graph_vectorizer import GraphVectorizer\n",
    "gv = GraphVectorizer(\n",
    "    graph_model_name=\"BAAI/bge-large-en-v1.5\", \n",
    "    device=None, \n",
    "    ner_backend=\"scispacy\", \n",
    "    min_df=1\n",
    ")\n",
    "gv.fit_corpus([(d.title or \"\") + \" \" + (d.text or \"\") for d in docs])\n",
    "v_g = gv.encode_text(queries[0].text)\n",
    "print(f\"\\nGraph (BGE-Large):\")\n",
    "print(f\"   Dim: {v_g.shape[0]:>4}d | L2 norm: {np.linalg.norm(v_g):.6f} | Sparsity: {np.sum(v_g == 0) / len(v_g) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n‚úì Todos os vetores est√£o normalizados (L2 norm ‚âà 1.0)\")\n",
    "print(\"‚úì TF-IDF √© esparso; Dense e Graph s√£o densos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4dfb9",
   "metadata": {},
   "source": [
    "### üìä Interpreta√ß√£o das Dimens√µes\n",
    "\n",
    "**TF-IDF** (1000d):\n",
    "- Dimens√£o = tamanho do vocabul√°rio (limitado por `max_features`)\n",
    "- **Altamente esparso**: maioria das dimens√µes √© zero (termos n√£o presentes)\n",
    "- Sparsity t√≠pica: 95-99%\n",
    "\n",
    "**Dense MiniLM** (384d):\n",
    "- Dimens√£o fixa definida pelo modelo\n",
    "- **Totalmente denso**: todas as dimens√µes t√™m valores n√£o-zero\n",
    "- Menor que BGE, mas ainda captura sem√¢ntica eficazmente\n",
    "\n",
    "**Graph BGE-Large** (1024d):\n",
    "- Dimens√£o = dimens√£o do encoder de entidades\n",
    "- **Denso**: composi√ß√£o ponderada de embeddings de entidades\n",
    "- Maior dimens√£o ‚Üí maior capacidade representacional\n",
    "\n",
    "**Normaliza√ß√£o L2**: Todos os vetores t√™m norma ‚âà 1.0, permitindo compara√ß√£o direta por inner-product (equivalente a cosseno)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
