{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DAT Dataset Generation: SQuAD Reduced Dataset\n",
        "\n",
        "Este notebook gera um dataset reduzido inspirado no paper DAT (Dynamic Alpha Tuning) a partir do SQuAD v1.1 j√° convertido para formato BEIR.\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Criar um dataset reduzido com:\n",
        "- **Corpus reduzido**: ~585 par√°grafos\n",
        "- **Queries compat√≠veis**: ~2.976 perguntas cujas respostas est√£o nesses par√°grafos\n",
        "- **Subset Hybrid-Sensitive**: ~1.111 queries onde BM25 top-1 ‚â† Dense top-1\n",
        "\n",
        "Tudo salvo em `data/squad_small/processed/beir/` seguindo o formato BEIR padr√£o.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration\n",
        "\n",
        "Configura√ß√£o inicial: imports, constantes e paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Setup complete\n",
            "   Input:  /Users/thiago/Documents/GitHub/hybrid-retrieval/data/squad/processed/beir\n",
            "   Output: /Users/thiago/Documents/GitHub/hybrid-retrieval/data/squad_small/processed/beir\n",
            "   Seed:   42\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import json\n",
        "from datetime import datetime\n",
        "import random\n",
        "\n",
        "# Add repo root to path\n",
        "repo_root = Path.cwd().parent if Path.cwd().name == \"experiments\" else Path.cwd()\n",
        "repo_root_str = str(repo_root)\n",
        "if repo_root_str not in sys.path:\n",
        "    sys.path.insert(0, repo_root_str)\n",
        "\n",
        "from src.datasets.loader import load_beir_dataset, as_documents, as_queries\n",
        "from src.retrievers.bm25_basic import BM25Basic\n",
        "from src.retrievers.dense_faiss import DenseFaiss\n",
        "\n",
        "# Configuration constants\n",
        "SEED = 42\n",
        "TARGET_DOCS = 585\n",
        "EXPECTED_QUERIES = 2976\n",
        "EXPECTED_HYBRID_SENSITIVE = 1111\n",
        "K = 20  # Retrieval depth for hybrid-sensitive identification\n",
        "\n",
        "# Paths\n",
        "input_root = repo_root / \"data\" / \"squad\" / \"processed\" / \"beir\"\n",
        "output_root = repo_root / \"data\" / \"squad_small\" / \"processed\" / \"beir\"\n",
        "output_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dense model configuration (can be changed)\n",
        "DENSE_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Local model by default\n",
        "# Alternative: \"text-embedding-3-large\" with provider=\"openai\" (requires API key)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(SEED)\n",
        "if hasattr(pd, 'numpy'):\n",
        "    import numpy as np\n",
        "    np.random.seed(SEED)\n",
        "\n",
        "print(f\"‚úÖ Setup complete\")\n",
        "print(f\"   Input:  {input_root}\")\n",
        "print(f\"   Output: {output_root}\")\n",
        "print(f\"   Seed:   {SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Stage A: Load & Validate Input Data\n",
        "\n",
        "Carregar e validar os dados do SQuAD original.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading SQuAD dataset...\n",
            "\n",
            "‚úÖ Loaded:\n",
            "   Corpus:  20,958 documents\n",
            "   Queries: 98,169 queries\n",
            "   Qrels:   98,169 pairs\n",
            "\n",
            "üîç Validating structure...\n",
            "\n",
            "üîç Checking uniqueness...\n",
            "\n",
            "üîç Checking qrels integrity...\n",
            "\n",
            "üîç Checking data quality...\n",
            "\n",
            "üìä Computing input file hashes...\n",
            "   corpus.parquet: e4066c82e40d3c30...\n",
            "   queries.parquet: f4b77397b6e47091...\n",
            "   qrels.parquet: add4d02020230b05...\n",
            "\n",
            "‚úÖ Stage A complete: Input data validated\n"
          ]
        }
      ],
      "source": [
        "def md5sum(path: Path) -> str:\n",
        "    \"\"\"Compute MD5 hash of a file.\"\"\"\n",
        "    h = hashlib.md5()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# Load SQuAD dataset\n",
        "print(\"üì• Loading SQuAD dataset...\")\n",
        "corpus, queries, qrels = load_beir_dataset(input_root)\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded:\")\n",
        "print(f\"   Corpus:  {len(corpus):,} documents\")\n",
        "print(f\"   Queries: {len(queries):,} queries\")\n",
        "print(f\"   Qrels:   {len(qrels):,} pairs\")\n",
        "\n",
        "# Validate columns\n",
        "print(\"\\nüîç Validating structure...\")\n",
        "required_corpus_cols = [\"doc_id\", \"title\", \"text\"]\n",
        "required_queries_cols = [\"query_id\", \"query\"]\n",
        "required_qrels_cols = [\"query_id\", \"doc_id\", \"score\", \"split\"]\n",
        "\n",
        "missing_corpus = set(required_corpus_cols) - set(corpus.columns)\n",
        "missing_queries = set(required_queries_cols) - set(queries.columns)\n",
        "missing_qrels = set(required_qrels_cols) - set(qrels.columns)\n",
        "\n",
        "if missing_corpus or missing_queries or missing_qrels:\n",
        "    raise ValueError(f\"Missing columns: corpus={missing_corpus}, queries={missing_queries}, qrels={missing_qrels}\")\n",
        "\n",
        "# Check uniqueness\n",
        "print(\"\\nüîç Checking uniqueness...\")\n",
        "duplicate_docs = corpus[\"doc_id\"].duplicated().sum()\n",
        "duplicate_queries = queries[\"query_id\"].duplicated().sum()\n",
        "\n",
        "if duplicate_docs > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {duplicate_docs} duplicate doc_ids found\")\n",
        "if duplicate_queries > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {duplicate_queries} duplicate query_ids found\")\n",
        "\n",
        "# Verify 1 qrel per query (handle duplicates)\n",
        "print(\"\\nüîç Checking qrels integrity...\")\n",
        "qrels_per_query = qrels.groupby(\"query_id\").size()\n",
        "multi_qrels = (qrels_per_query > 1).sum()\n",
        "if multi_qrels > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {multi_qrels} queries have multiple qrels\")\n",
        "    print(f\"   Keeping first qrel for each query\")\n",
        "    qrels = qrels.drop_duplicates(subset=[\"query_id\"], keep=\"first\")\n",
        "    print(f\"   Qrels after deduplication: {len(qrels):,}\")\n",
        "\n",
        "# Check for nulls/empty strings\n",
        "print(\"\\nüîç Checking data quality...\")\n",
        "null_docs = corpus[[\"doc_id\", \"text\"]].isnull().any(axis=1).sum()\n",
        "null_queries = queries[[\"query_id\", \"query\"]].isnull().any(axis=1).sum()\n",
        "empty_texts = (corpus[\"text\"].astype(str).str.strip() == \"\").sum()\n",
        "empty_queries = (queries[\"query\"].astype(str).str.strip() == \"\").sum()\n",
        "\n",
        "if null_docs > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {null_docs} documents with null critical fields\")\n",
        "if null_queries > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {null_queries} queries with null critical fields\")\n",
        "if empty_texts > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {empty_texts} documents with empty text\")\n",
        "if empty_queries > 0:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {empty_queries} queries with empty query\")\n",
        "\n",
        "# Compute input file hashes\n",
        "print(\"\\nüìä Computing input file hashes...\")\n",
        "input_hashes = {}\n",
        "for file_name in [\"corpus.parquet\", \"queries.parquet\", \"qrels.parquet\"]:\n",
        "    file_path = input_root / file_name\n",
        "    if file_path.exists():\n",
        "        hash_val = md5sum(file_path)\n",
        "        input_hashes[file_name.replace(\".parquet\", \"\")] = hash_val\n",
        "        print(f\"   {file_name}: {hash_val[:16]}...\")\n",
        "\n",
        "print(\"\\n‚úÖ Stage A complete: Input data validated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Stage B: Reduce Corpus (~585 paragraphs)\n",
        "\n",
        "Reduzir o corpus agrupando por t√≠tulo e selecionando t√≠tulos at√© alcan√ßar ~585 documentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìâ Reducing corpus to ~585 documents...\n",
            "\n",
            "üìä Title distribution:\n",
            "   Total titles: 490\n",
            "   Total paragraphs: 20958\n",
            "   Avg paragraphs per title: 42.8\n",
            "   Max paragraphs per title: 149\n",
            "   Min paragraphs per title: 5\n",
            "\n",
            "üìã Selected 5 titles\n",
            "   Cumulative paragraphs: 621\n",
            "\n",
            "‚úÇÔ∏è  Trimming from 621 to 585 documents...\n",
            "\n",
            "‚úÖ Stage B complete:\n",
            "   Selected documents: 585\n",
            "   Sampling method: by_title_trimmed\n"
          ]
        }
      ],
      "source": [
        "print(\"üìâ Reducing corpus to ~585 documents...\")\n",
        "\n",
        "# Group by title\n",
        "title_groups = corpus.groupby(\"title\", dropna=False)\n",
        "title_counts = title_groups.size().sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nüìä Title distribution:\")\n",
        "print(f\"   Total titles: {len(title_counts)}\")\n",
        "print(f\"   Total paragraphs: {len(corpus)}\")\n",
        "print(f\"   Avg paragraphs per title: {title_counts.mean():.1f}\")\n",
        "print(f\"   Max paragraphs per title: {title_counts.max()}\")\n",
        "print(f\"   Min paragraphs per title: {title_counts.min()}\")\n",
        "\n",
        "# Strategy: Select titles until we reach ~TARGET_DOCS\n",
        "selected_titles = []\n",
        "cumulative_count = 0\n",
        "\n",
        "for title, count in title_counts.items():\n",
        "    if cumulative_count + count <= TARGET_DOCS * 1.1:  # Allow 10% over\n",
        "        selected_titles.append(title)\n",
        "        cumulative_count += count\n",
        "        if cumulative_count >= TARGET_DOCS:\n",
        "            break\n",
        "    else:\n",
        "        # If adding this title would exceed too much, skip it\n",
        "        continue\n",
        "\n",
        "print(f\"\\nüìã Selected {len(selected_titles)} titles\")\n",
        "print(f\"   Cumulative paragraphs: {cumulative_count}\")\n",
        "\n",
        "# Filter corpus to selected titles\n",
        "corpus_selected = corpus[corpus[\"title\"].isin(selected_titles)].copy()\n",
        "\n",
        "# If we're over target, trim deterministically\n",
        "if len(corpus_selected) > TARGET_DOCS:\n",
        "    print(f\"\\n‚úÇÔ∏è  Trimming from {len(corpus_selected)} to {TARGET_DOCS} documents...\")\n",
        "    corpus_selected = corpus_selected.sort_values(\"doc_id\").head(TARGET_DOCS)\n",
        "    sampling_method = \"by_title_trimmed\"\n",
        "elif len(corpus_selected) < TARGET_DOCS * 0.9:  # If less than 90% of target\n",
        "    print(f\"\\n‚ö†Ô∏è  Only {len(corpus_selected)} docs selected, falling back to random sampling...\")\n",
        "    corpus_selected = corpus.sample(n=TARGET_DOCS, random_state=SEED)\n",
        "    sampling_method = \"random\"\n",
        "else:\n",
        "    sampling_method = \"by_title\"\n",
        "\n",
        "DOCS_STAR = set(corpus_selected[\"doc_id\"].tolist())\n",
        "\n",
        "print(f\"\\n‚úÖ Stage B complete:\")\n",
        "print(f\"   Selected documents: {len(DOCS_STAR)}\")\n",
        "print(f\"   Sampling method: {sampling_method}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Stage C: Select Consistent Queries (~2,976)\n",
        "\n",
        "Filtrar queries para manter apenas aquelas cujo par√°grafo relevante est√° no corpus reduzido.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Filtering queries to match reduced corpus...\n",
            "\n",
            "üìä Filter results:\n",
            "   Qrels matching reduced corpus: 2,823\n",
            "   Unique queries: 2,823\n",
            "\n",
            "‚úÖ Stage C complete:\n",
            "   Corpus (small):  585 documents\n",
            "   Queries (small): 2,823 queries\n",
            "   Qrels (small):   2,823 pairs\n",
            "   Expected queries: ~2976\n"
          ]
        }
      ],
      "source": [
        "print(\"üîç Filtering queries to match reduced corpus...\")\n",
        "\n",
        "# Filter qrels to only include doc_id in DOCS_STAR\n",
        "qrels_filtered = qrels[qrels[\"doc_id\"].isin(DOCS_STAR)].copy()\n",
        "\n",
        "# Get unique query_ids from filtered qrels\n",
        "valid_query_ids = set(qrels_filtered[\"query_id\"].unique())\n",
        "\n",
        "print(f\"\\nüìä Filter results:\")\n",
        "print(f\"   Qrels matching reduced corpus: {len(qrels_filtered):,}\")\n",
        "print(f\"   Unique queries: {len(valid_query_ids):,}\")\n",
        "\n",
        "# Verify 1 qrel per query in filtered set\n",
        "qrels_per_query_filtered = qrels_filtered.groupby(\"query_id\").size()\n",
        "multi_qrels_filtered = (qrels_per_query_filtered > 1).sum()\n",
        "if multi_qrels_filtered > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  Warning: {multi_qrels_filtered} queries have multiple qrels in filtered set\")\n",
        "    print(f\"   Keeping first qrel for each query\")\n",
        "    qrels_filtered = qrels_filtered.drop_duplicates(subset=[\"query_id\"], keep=\"first\")\n",
        "    valid_query_ids = set(qrels_filtered[\"query_id\"].unique())\n",
        "\n",
        "# Filter queries DataFrame\n",
        "queries_small = queries[queries[\"query_id\"].isin(valid_query_ids)].copy()\n",
        "\n",
        "# Filter corpus to DOCS_STAR\n",
        "corpus_small = corpus[corpus[\"doc_id\"].isin(DOCS_STAR)].copy()\n",
        "\n",
        "# Final qrels (already filtered)\n",
        "qrels_small = qrels_filtered.copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Stage C complete:\")\n",
        "print(f\"   Corpus (small):  {len(corpus_small):,} documents\")\n",
        "print(f\"   Queries (small): {len(queries_small):,} queries\")\n",
        "print(f\"   Qrels (small):   {len(qrels_small):,} pairs\")\n",
        "print(f\"   Expected queries: ~{EXPECTED_QUERIES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Stage D: Sanity Checks\n",
        "\n",
        "Revalidar invariantes e calcular estat√≠sticas do dataset reduzido.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Running sanity checks on reduced dataset...\n",
            "\n",
            "1. Integrity checks:\n",
            "   ‚úÖ All qrels doc_ids exist in corpus\n",
            "   ‚úÖ All qrels query_ids exist in queries\n",
            "   ‚úÖ Exactly 1 qrel per query\n",
            "\n",
            "2. Uniqueness checks:\n",
            "   ‚úÖ No duplicate doc_ids or query_ids\n",
            "\n",
            "3. Statistics:\n",
            "   Final counts:\n",
            "      Documents: 585\n",
            "      Queries:   2,823\n",
            "      Qrels:     2,823\n",
            "\n",
            "   Split distribution:\n",
            "      train: 2,344 (83.0%)\n",
            "      test: 479 (17.0%)\n",
            "\n",
            "   Average lengths:\n",
            "      Document text: 629 characters\n",
            "      Query text:    57 characters\n",
            "\n",
            "‚úÖ Stage D complete: All sanity checks passed\n"
          ]
        }
      ],
      "source": [
        "print(\"üîç Running sanity checks on reduced dataset...\")\n",
        "\n",
        "# Re-validate invariants\n",
        "print(\"\\n1. Integrity checks:\")\n",
        "# All doc_ids in qrels exist in corpus\n",
        "qrels_doc_ids = set(qrels_small[\"doc_id\"].unique())\n",
        "corpus_doc_ids = set(corpus_small[\"doc_id\"].unique())\n",
        "missing_docs = qrels_doc_ids - corpus_doc_ids\n",
        "if missing_docs:\n",
        "    print(f\"   ‚ùå ERROR: {len(missing_docs)} doc_ids in qrels not found in corpus\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ All qrels doc_ids exist in corpus\")\n",
        "\n",
        "# All query_ids in qrels exist in queries\n",
        "qrels_query_ids = set(qrels_small[\"query_id\"].unique())\n",
        "queries_query_ids = set(queries_small[\"query_id\"].unique())\n",
        "missing_queries = qrels_query_ids - queries_query_ids\n",
        "if missing_queries:\n",
        "    print(f\"   ‚ùå ERROR: {len(missing_queries)} query_ids in qrels not found in queries\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ All qrels query_ids exist in queries\")\n",
        "\n",
        "# 1 qrel per query\n",
        "qrels_per_query = qrels_small.groupby(\"query_id\").size()\n",
        "if (qrels_per_query > 1).any():\n",
        "    print(f\"   ‚ùå ERROR: Some queries have multiple qrels\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Exactly 1 qrel per query\")\n",
        "\n",
        "# Uniqueness\n",
        "print(\"\\n2. Uniqueness checks:\")\n",
        "duplicate_docs = corpus_small[\"doc_id\"].duplicated().sum()\n",
        "duplicate_queries = queries_small[\"query_id\"].duplicated().sum()\n",
        "if duplicate_docs == 0 and duplicate_queries == 0:\n",
        "    print(f\"   ‚úÖ No duplicate doc_ids or query_ids\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Duplicates: docs={duplicate_docs}, queries={duplicate_queries}\")\n",
        "\n",
        "# Statistics\n",
        "print(\"\\n3. Statistics:\")\n",
        "print(f\"   Final counts:\")\n",
        "print(f\"      Documents: {len(corpus_small):,}\")\n",
        "print(f\"      Queries:   {len(queries_small):,}\")\n",
        "print(f\"      Qrels:     {len(qrels_small):,}\")\n",
        "\n",
        "# Split distribution\n",
        "if \"split\" in qrels_small.columns:\n",
        "    split_counts = qrels_small[\"split\"].value_counts()\n",
        "    print(f\"\\n   Split distribution:\")\n",
        "    for split, count in split_counts.items():\n",
        "        print(f\"      {split}: {count:,} ({count/len(qrels_small)*100:.1f}%)\")\n",
        "\n",
        "# Average text lengths\n",
        "avg_doc_chars = corpus_small[\"text\"].astype(str).str.len().mean()\n",
        "avg_query_chars = queries_small[\"query\"].astype(str).str.len().mean()\n",
        "print(f\"\\n   Average lengths:\")\n",
        "print(f\"      Document text: {avg_doc_chars:.0f} characters\")\n",
        "print(f\"      Query text:    {avg_query_chars:.0f} characters\")\n",
        "\n",
        "print(\"\\n‚úÖ Stage D complete: All sanity checks passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Stage E: Generate Hybrid-Sensitive Subset\n",
        "\n",
        "Executar retrieval BM25 e Dense para identificar queries onde top-1 BM25 ‚â† top-1 Dense.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Generating Hybrid-Sensitive subset...\n",
            "   This will run BM25 and Dense retrieval on 2,823 queries...\n",
            "\n",
            "üì¶ Converted to objects:\n",
            "   Documents: 585\n",
            "   Queries:   2823\n",
            "\n",
            "üîß Initializing retrievers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/thiago/Documents/GitHub/hybrid-retrieval/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèóÔ∏è  Building indexes...\n",
            "   Building BM25 index...\n",
            "   Building Dense index...\n",
            "2025-11-05 21:42:26 | INFO     | retriever.dense | [dense_faiss.py:79] | üöÄ Building Dense Index (585 documentos)\n",
            "2025-11-05 21:42:26 | INFO     | retriever.dense | [logging.py:199] | ‚è±Ô∏è  Encoding documents - iniciando...\n",
            "2025-11-05 21:42:33 | INFO     | retriever.dense | [logging.py:220] | ‚úì Encoding documents - conclu√≠do em \u001b[32m6.97s\u001b[0m\n",
            "2025-11-05 21:42:33 | INFO     | retriever.dense | [logging.py:199] | ‚è±Ô∏è  Construindo FAISS IndexFlatIP - iniciando...\n",
            "2025-11-05 21:42:33 | INFO     | retriever.dense | [logging.py:220] | ‚úì Construindo FAISS IndexFlatIP - conclu√≠do em \u001b[32m0.6ms\u001b[0m\n",
            "2025-11-05 21:42:33 | INFO     | retriever.dense | [dense_faiss.py:108] |   ‚úì FAISS IndexFlatIP: 585 vetores, dim=384\n",
            "‚úÖ Indexes built\n"
          ]
        }
      ],
      "source": [
        "print(\"üîç Generating Hybrid-Sensitive subset...\")\n",
        "print(f\"   This will run BM25 and Dense retrieval on {len(queries_small):,} queries...\")\n",
        "\n",
        "# Convert to Document and Query objects\n",
        "documents = as_documents(corpus_small)\n",
        "query_objects = as_queries(queries_small)\n",
        "\n",
        "print(f\"\\nüì¶ Converted to objects:\")\n",
        "print(f\"   Documents: {len(documents)}\")\n",
        "print(f\"   Queries:   {len(query_objects)}\")\n",
        "\n",
        "# Initialize retrievers\n",
        "print(\"\\nüîß Initializing retrievers...\")\n",
        "bm25_retriever = BM25Basic(k1=0.9, b=0.4)\n",
        "dense_retriever = DenseFaiss(\n",
        "    model_name=DENSE_MODEL,\n",
        "    use_faiss=True,\n",
        "    artifact_dir=str(output_root.parent.parent / \"artifacts\" / \"squad_small_dense\"),\n",
        "    index_name=\"dense.index\"\n",
        ")\n",
        "\n",
        "# Build indexes\n",
        "print(\"\\nüèóÔ∏è  Building indexes...\")\n",
        "print(\"   Building BM25 index...\")\n",
        "bm25_retriever.build_index(documents)\n",
        "\n",
        "print(\"   Building Dense index...\")\n",
        "dense_retriever.build_index(documents)\n",
        "\n",
        "print(\"‚úÖ Indexes built\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Running retrieval (K=20) for all queries...\n",
            "   Processed 500 / 2,823 queries\n",
            "   Processed 1,000 / 2,823 queries\n",
            "   Processed 1,500 / 2,823 queries\n",
            "   Processed 2,000 / 2,823 queries\n",
            "   Processed 2,500 / 2,823 queries\n",
            "   Processed 2,823 / 2,823 queries\n",
            "‚úÖ Retrieval complete\n"
          ]
        }
      ],
      "source": [
        "# Run retrieval for all queries\n",
        "print(f\"\\nüîç Running retrieval (K={K}) for all queries...\")\n",
        "\n",
        "# Process in batches to show progress\n",
        "batch_size = 100\n",
        "all_bm25_results = {}\n",
        "all_dense_results = {}\n",
        "\n",
        "for i in range(0, len(query_objects), batch_size):\n",
        "    batch_queries = query_objects[i:i+batch_size]\n",
        "    \n",
        "    # BM25 retrieval\n",
        "    bm25_results = bm25_retriever.retrieve(batch_queries, k=K)\n",
        "    all_bm25_results.update(bm25_results)\n",
        "    \n",
        "    # Dense retrieval\n",
        "    dense_results = dense_retriever.retrieve(batch_queries, k=K)\n",
        "    all_dense_results.update(dense_results)\n",
        "    \n",
        "    if (i + batch_size) % 500 == 0 or (i + batch_size) >= len(query_objects):\n",
        "        print(f\"   Processed {min(i + batch_size, len(query_objects)):,} / {len(query_objects):,} queries\")\n",
        "\n",
        "print(\"‚úÖ Retrieval complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Extracting top-1 results...\n",
            "\n",
            "‚úÖ Hybrid-Sensitive identification complete:\n",
            "   Total queries analyzed: 2,823\n",
            "   Hybrid-Sensitive queries: 1,273\n",
            "   Proportion: 45.1%\n",
            "   Expected: ~1111\n",
            "\n",
            "üìä Retrievability analysis:\n",
            "   Ground truth not in BM25@20: 148 queries\n",
            "   Ground truth not in Dense@20: 109 queries\n",
            "\n",
            "‚úÖ Stage E complete:\n",
            "   Hybrid-Sensitive queries: 1,273\n",
            "   Hybrid-Sensitive qrels:   1,273\n"
          ]
        }
      ],
      "source": [
        "# Extract top-1 from each method\n",
        "print(\"\\nüìä Extracting top-1 results...\")\n",
        "\n",
        "# Get all query IDs from query_objects to ensure we check all queries\n",
        "all_query_ids = {q.query_id for q in query_objects}\n",
        "\n",
        "top1_bm25 = {}\n",
        "top1_dense = {}\n",
        "\n",
        "for query_id in all_query_ids:\n",
        "    bm25_results = all_bm25_results.get(query_id, [])\n",
        "    dense_results = all_dense_results.get(query_id, [])\n",
        "    \n",
        "    top1_bm25[query_id] = bm25_results[0][0] if bm25_results else None\n",
        "    top1_dense[query_id] = dense_results[0][0] if dense_results else None\n",
        "\n",
        "# Identify Hybrid-Sensitive queries: top1_bm25 ‚â† top1_dense\n",
        "hybrid_sensitive_query_ids = []\n",
        "for query_id in top1_bm25.keys():\n",
        "    if top1_bm25[query_id] != top1_dense[query_id]:\n",
        "        hybrid_sensitive_query_ids.append(query_id)\n",
        "\n",
        "hybrid_sensitive_query_ids = set(hybrid_sensitive_query_ids)\n",
        "\n",
        "print(f\"\\n‚úÖ Hybrid-Sensitive identification complete:\")\n",
        "print(f\"   Total queries analyzed: {len(top1_bm25):,}\")\n",
        "print(f\"   Hybrid-Sensitive queries: {len(hybrid_sensitive_query_ids):,}\")\n",
        "print(f\"   Proportion: {len(hybrid_sensitive_query_ids)/len(top1_bm25)*100:.1f}%\")\n",
        "print(f\"   Expected: ~{EXPECTED_HYBRID_SENSITIVE}\")\n",
        "\n",
        "# Check retrievability (ground truth in top-K)\n",
        "print(\"\\nüìä Retrievability analysis:\")\n",
        "ground_truth_map = dict(zip(qrels_small[\"query_id\"], qrels_small[\"doc_id\"]))\n",
        "\n",
        "unretrievable_bm25 = 0\n",
        "unretrievable_dense = 0\n",
        "\n",
        "for query_id in hybrid_sensitive_query_ids:\n",
        "    gt_doc = ground_truth_map.get(query_id)\n",
        "    if gt_doc:\n",
        "        bm25_topk = [r[0] for r in all_bm25_results.get(query_id, [])]\n",
        "        dense_topk = [r[0] for r in all_dense_results.get(query_id, [])]\n",
        "        \n",
        "        if gt_doc not in bm25_topk:\n",
        "            unretrievable_bm25 += 1\n",
        "        if gt_doc not in dense_topk:\n",
        "            unretrievable_dense += 1\n",
        "\n",
        "print(f\"   Ground truth not in BM25@20: {unretrievable_bm25:,} queries\")\n",
        "print(f\"   Ground truth not in Dense@20: {unretrievable_dense:,} queries\")\n",
        "\n",
        "# Filter to Hybrid-Sensitive subset\n",
        "queries_hybrid = queries_small[queries_small[\"query_id\"].isin(hybrid_sensitive_query_ids)].copy()\n",
        "qrels_hybrid = qrels_small[qrels_small[\"query_id\"].isin(hybrid_sensitive_query_ids)].copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Stage E complete:\")\n",
        "print(f\"   Hybrid-Sensitive queries: {len(queries_hybrid):,}\")\n",
        "print(f\"   Hybrid-Sensitive qrels:   {len(qrels_hybrid):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Stage F: Write Artifacts & Metadata\n",
        "\n",
        "Salvar todos os arquivos parquet e gerar METADATA.json com informa√ß√µes de rastreabilidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Writing artifacts...\n",
            "   ‚úÖ Saved corpus.parquet (585 rows)\n",
            "   ‚úÖ Saved queries.parquet (2,823 rows)\n",
            "   ‚úÖ Saved qrels.parquet (2,823 rows)\n",
            "   ‚úÖ Saved queries_hybrid.parquet (1,273 rows)\n",
            "   ‚úÖ Saved qrels_hybrid.parquet (1,273 rows)\n",
            "\n",
            "‚úÖ All parquet files saved\n"
          ]
        }
      ],
      "source": [
        "print(\"üíæ Writing artifacts...\")\n",
        "\n",
        "# Write parquet files\n",
        "output_files = {\n",
        "    \"corpus.parquet\": corpus_small,\n",
        "    \"queries.parquet\": queries_small,\n",
        "    \"qrels.parquet\": qrels_small,\n",
        "    \"queries_hybrid.parquet\": queries_hybrid,\n",
        "    \"qrels_hybrid.parquet\": qrels_hybrid,\n",
        "}\n",
        "\n",
        "for filename, df in output_files.items():\n",
        "    filepath = output_root / filename\n",
        "    df.to_parquet(filepath, index=False, engine=\"pyarrow\")\n",
        "    print(f\"   ‚úÖ Saved {filename} ({len(df):,} rows)\")\n",
        "\n",
        "print(\"\\n‚úÖ All parquet files saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ METADATA.json saved\n",
            "\n",
            "üìã Metadata summary:\n",
            "   Seed: 42\n",
            "   Sampling method: by_title_trimmed\n",
            "   Final counts: {'docs': 585, 'queries': 2823, 'queries_hybrid': 1273}\n",
            "   Hybrid-Sensitive: 1273 queries\n",
            "   Created at: 2025-11-05T21:47:05.453831\n",
            "\n",
            "‚úÖ Stage F complete: All artifacts saved to /Users/thiago/Documents/GitHub/hybrid-retrieval/data/squad_small/processed/beir\n"
          ]
        }
      ],
      "source": [
        "# Generate METADATA.json\n",
        "metadata = {\n",
        "    \"seed\": SEED,\n",
        "    \"sampling_method\": sampling_method,\n",
        "    \"target_docs\": TARGET_DOCS,\n",
        "    \"final_counts\": {\n",
        "        \"docs\": len(corpus_small),\n",
        "        \"queries\": len(queries_small),\n",
        "        \"queries_hybrid\": len(queries_hybrid)\n",
        "    },\n",
        "    \"k\": K,\n",
        "    \"retrieval_models\": {\n",
        "        \"bm25\": {\n",
        "            \"k1\": 0.9,\n",
        "            \"b\": 0.4\n",
        "        },\n",
        "        \"dense\": {\n",
        "            \"model\": DENSE_MODEL,\n",
        "            \"provider\": \"huggingface\"  # Change if using OpenAI\n",
        "        }\n",
        "    },\n",
        "    \"created_at\": datetime.now().isoformat(),\n",
        "    \"input_hashes\": input_hashes,\n",
        "    \"hybrid_sensitive_stats\": {\n",
        "        \"count\": len(hybrid_sensitive_query_ids),\n",
        "        \"proportion\": len(hybrid_sensitive_query_ids) / len(top1_bm25) if top1_bm25 else 0,\n",
        "        \"unretrievable_bm25\": unretrievable_bm25,\n",
        "        \"unretrievable_dense\": unretrievable_dense\n",
        "    }\n",
        "}\n",
        "\n",
        "# Write METADATA.json\n",
        "metadata_path = output_root / \"METADATA.json\"\n",
        "with open(metadata_path, \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ METADATA.json saved\")\n",
        "print(f\"\\nüìã Metadata summary:\")\n",
        "print(f\"   Seed: {metadata['seed']}\")\n",
        "print(f\"   Sampling method: {metadata['sampling_method']}\")\n",
        "print(f\"   Final counts: {metadata['final_counts']}\")\n",
        "print(f\"   Hybrid-Sensitive: {metadata['hybrid_sensitive_stats']['count']} queries\")\n",
        "print(f\"   Created at: {metadata['created_at']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Stage F complete: All artifacts saved to {output_root}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
